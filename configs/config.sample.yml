# Copy the following to: ./configs/config.yaml

# Database Lab API server configuration.
# This API is used to serve requests such as "create a clone".
# Normally, 127.0.0.1:2345 is used to serve API requests,
# it is supposed to be running inside a Docker container,
# .... (example, what users will use)
server:
  # ...
  verificationToken: "secret_token"

  # The host which the Database Lab server accepts HTTP connections from. 
  # By default: "127.0.0.1", accepts only local connections. Use an empty string to accept all connections.
  host: "127.0.0.1"

  # HTTP server port . Default: 2345.
  port: 2345

global:
  # database engine ... Currently supported: "postgres"
  engine: postgres
  # ...image for database engine
  dockerImage: "postgres:12-alpine"
  # Must already exist before launching Database Lab server
  # For example, in the case of ZFS, ... sudo zpool ... /var/lib/dblab

  ## TODO rework:
  #      mountDir + pgDataSubDir, both right here
  #      and clonesMountDir
  dataDir: /var/lib/dblab/data

# Enable if more details in the logs are needed for troubleshooting
debug: false

provision:
  # Provision mode to use.
  # supported: "local" (only)
  # ??? TODO: make it hidden
  mode: "local"

  # Subdir where PGDATA located relative to the pool root dir.
  # relevant to global.dataDir, *with* leading slashes
  # If PGDATA is supposed to be in global.dataDir, then use "/" her
  # If PGDATA is deeper, specify part of the path, with leading slash. Example:
  #      dataDir (mount point):  /dblab/bigdisk
  #      pgDataSubdir:  /pgdata
  # -- in this case, PGDATA will be located at /dblab/bigdisk
  # ... TODO:  (1) trailing slash for all directories;  (2) leading slasg is a bad thing? we could auto-add it during concatenation
  pgDataSubdir: "/"

  # Username that will be used for Postgres management connections.
  # The user must exist.
  # ...Pasword is not needed - it will be set automatically, and connections are via Unix domain socket (local)
  # ...? TODO: do we need it at all ?!
  pgMgmtUsername: "postgres"

  # "Local" mode related parameters.
  local:
    # Which thin-clone managing module to use.
    # Available options: "zfs", "lvm".
    thinCloneManager: "zfs"

    # Name of your pool (in the case of ZFS) or volume group
    # with logic volume name (e.g. "dblab_vg/pg_lv", in the case of LVM).
    pool: "dblab_pool"

    # Pool of ports for Postgres clones.
    portPool:
      from: 6000
      to: 6100

    # Clones PGDATA mount directory.
    # ... TODO include "clone" in the param name?
    mountDir: /var/lib/dblab/clones

    # Unix sockets directory for secure connection to Postgres clones.
    unixSocketDir: /var/lib/dblab/sockets

    # Snapshots with the suffix will not be accessible to use for cloning.
    snapshotFilterSuffix: "_pre"

    # Database Lab provisions thin clones using Docker containers, we need
    # to specify which Postgres Docker image is to be used when cloning.
    # The default is the extended Postgres image built on top of the official Postgres image
    # (See https://hub.docker.com/repository/docker/postgresai/extended-postgres).
    # Any custom or official Docker image that runs Postgres with PGDATA located
    # in "/var/lib/postgresql/pgdata" directory. Our Dockerfile
    # (See https://gitlab.com/postgres-ai/database-lab/snippets/1932037)
    # is recommended in case if customization is needed.
    dockerImage: "postgresai/extended-postgres:12"

    # Use sudo for ZFS/LVM and Docker commands if Database Lab server running
    # outside a container.
    useSudo: false

retrieval:
  stages:
    - initialize

  spec:
    # The initialize stage provides declarative initialization of the PostgreSQL data directory used by Database Lab Engine.
    # The stage must not contain physical and logical restore jobs simultaneously.
    initialize:
      jobs:
        # Dumps PostgreSQL database from provided source.
        - name: logical-dump
          options:
            # The dump file will be automatically created on this location and then used to restore.
            dumpLocation: /tmp/db.dump

            # The Docker image containing the tools required to get a dump.
            dockerImage: "postgres:12-alpine"

            # Source of data.
            source:
              # Source type: local, remote, rds.
              type: local

              # Connection parameters of the database to be dumped.
              connection:
                dbname: postgres
                host: 127.0.0.1
                port: 5432
                username: postgres
                # Connection password. The environment variable PGPASSWORD can be used instead of this option.
                # The environment variable has a higher priority.
                password: postgres

              # Optional definition of RDS data source.
              rds:
                # RDS policy name.
                iamPolicyName: rds-dblab-retrieval

                # AWS Region.
                awsRegion: us-east-2

                # AWS username.
                username: db_user

                # RDS instance Identifier.
                dbInstanceIdentifier: database-1

                # Path to the SSL root certificate: https://s3.amazonaws.com/rds-downloads/rds-combined-ca-bundle.pem
                sslRootCert: "/tmp/rds-combined-ca-bundle.pem"

            # Options for a partial dump.
            partial:
              tables:
                - test

            # The number of parallel jobs to get a dump.
            # It's ignored if "restore" is present because "pg_dump | pg_restore" is always single-threaded.
            parallelJobs: 1

            # Options for direct restore to Database Lab Engine instance.
            restore:
              # Restore data even if the Postgres directory (`global.dataDir`) is not empty.
              # Note the existing data might be overwritten.
              forceInit: true

        # Restores PostgreSQL database from the provided dump.
        - name: logical-restore
          options:
            dbname: test
            # The location of the archive file (or directory, for a directory-format archive) to be restored.
            dumpLocation: /tmp/db.dump

            # Restore data even if the Postgres directory (`global.dataDir`) is not empty.
            # Note the existing data might be overwritten.
            forceInit: false

            # Options for a partial dump.
            partial:
              tables:
                - test

        # Restores database data from a physical backup.
        - name: physical-restore
          options:
            tool: walg
            dockerImage: "postgresai/sync-instance:12"
            envs:
              WALG_GS_PREFIX: "gs://{BUCKET}/{SCOPE}"
            walg:
              storage: gcs
              backupName: LATEST
              credentialsFile: /tmp/sa.json # optional

        - name: logical-snapshot
          options:
            # It is possible to define a pre-precessing script. For example, /tmp/scripts/custom.sh
            preprocessingScript: ""

            # Section for adding custom postgresql.conf parameters.
            configs:
              # In order to match production plans with Database Lab plans set parameters related to Query Planning as on production.

              # Be careful with memory consumption: besides shared_buffers (immediately allocated when the clone is started),
              # memory is consumed by Postgres backends directly (depends on work_mem and maintenance_work_mem values),
              # and by other applications.
              # If there is not enough memory to run more clones, then, depending on OS settings,
              # you may experience either OOM killer or swapping activity.
              # The "free" memory is automatically used for the file cache which is shared among clones.
              shared_buffers: 1GB
              # effective_cache_size: 64MB

        - name: physical-snapshot
          options:
            # Promote PGDATA after data fetching.
            promote: true

            # It is possible to define a pre-precessing script. For example, /tmp/scripts/custom.sh
            preprocessingScript: ""

            # Section for adding custom postgresql.conf parameters.
            configs:
              # In order to match production plans with Database Lab plans set parameters related to Query Planning as on production.
              shared_buffers: 1GB
              # effective_cache_size: 64MB

cloning:
  # base, mock ... TODO: make it hidden
  mode: "base"

  # Host that will be specified in database connection info for all clones
  # Use public IP address if database connections are allowed from outside
  # ... this value is only used to inform users about how to connect to database clones
  # ... // UX important
  accessHost: "localhost"

  # Automatically delete clones after the specified minutes of inactivity.
  # 0 - disable automatic deletion.
  # Inactivity means:
  #   - no active sessions (queries being processed right now)
  #   - no recently logged queries in the query log
  maxIdleMinutes: 120


# === INTEGRATION ===
# for open-source independent **.. remove all the following

# Postgres.ai Platform integration (... ... GUI)
# ...??? comment this block out ?
# or just advice: comment this out!
platform:
  # API URL. Do not change to work with Postgres.ai SaaS
  url: "https://postgres.ai/api/general"

  # Token for authorization in Platform API. This token can be obtained in Postgres.ai Console:
  # see the corresponding organization, "Access tokens" in the left menu.
  # ... URL ?? https://postgres.ai/console/YOUR_ORG_NAME/tokens
  accessToken: "platform_access_token"

  # Enable authorization with personal tokens of the organization's members.
  # If false: all users must use "accessToken" value for any API request
  # If true: "accessToken" is known only to admin, users use their own tokens,
  #          and any token can be revoked not affecting others
  enablePersonalTokens: true
