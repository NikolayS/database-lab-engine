# Copy the following to: ./configs/config.yaml

# Database Lab server configuration.
server:
  verificationToken: "secret_token"

  # The host which the Database Lab server accepts HTTP connections from. 
  # By default: "127.0.0.1", accepts only local connections. Use an empty string to accept all connections.
  host: "127.0.0.1"

  # HTTP server port. By default: 2345.
  port: 2345

global:
  engine: postgres
  dataDir: /var/lib/dblab
  dockerImage: "postgres:12-alpine"

# Postgres.ai Platform integration.
platform:
  # API URL.
  url: "https://postgres.ai/api/general"

  # Token for authorization in Platform API. This token can be obtained in Postgres.ai Console:
  # see the corresponding organization, "Access tokens" in the left menu.
  accessToken: "platform_access_token"

  # Enable authorization with personal tokens of the organization's members.
  enablePersonalTokens: false

provision:
  # Provision mode to use.
  mode: "local"

  # Subdir where PGDATA located relative to the pool root dir.
  pgDataSubdir: "/"

  # Username that will be used for Postgres management connections.
  # The user should exist.
  pgMgmtUsername: "postgres"

  # "Local" mode related parameters.
  local:
    # Which thin-clone managing module to use.
    # Available options: "zfs", "lvm".
    thinCloneManager: "zfs"

    # Name of your pool (in the case of ZFS) or volume group
    # with logic volume name (e.g. "dblab_vg/pg_lv", in the case of LVM).
    pool: "dblab_pool"

    # Pool of ports for Postgres clones.
    portPool:
      from: 6000
      to: 6100

    # Clones PGDATA mount directory.
    mountDir: /var/lib/dblab/clones

    # Unix sockets directory for secure connection to Postgres clones.
    unixSocketDir: /var/lib/dblab/sockets

    # Snapshots with the suffix will not be accessible to use for cloning.
    snapshotFilterSuffix: "_pre"

    # Database Lab provisions thin clones using Docker containers, we need
    # to specify which Postgres Docker image is to be used when cloning.
    # The default is the extended Postgres image built on top of the official Postgres image
    # (See https://hub.docker.com/repository/docker/postgresai/extended-postgres).
    # Any custom or official Docker image that runs Postgres with PGDATA located
    # in "/var/lib/postgresql/pgdata" directory. Our Dockerfile
    # (See https://gitlab.com/postgres-ai/database-lab/snippets/1932037)
    # is recommended in case if customization is needed.
    dockerImage: "postgresai/extended-postgres:12"

    # Use sudo for ZFS/LVM and Docker commands if Database Lab server running
    # outside a container.
    useSudo: false

cloning:
  mode: "base"

  # Host which will be specified in clone connection info.
  accessHost: "localhost"

  # Auto-delete clones after the specified minutes of inactivity.
  # 0 - disable automatic deletion.
  maxIdleMinutes: 120

retrieval:
  stages:
    - initialize

  spec:
    # The initialize stage provides declarative initialization of the PostgreSQL data directory used by Database Lab Engine.
    # The stage must not contain physical and logical restore jobs simultaneously.
    initialize:
      jobs:
        # Dumps PostgreSQL database from provided source.
        - name: logical-dump
          options:
            # The dump file will be automatically created on this location and then used to restore.
            dumpLocation: /tmp/db.dump

            # The Docker image containing the tools required to get a dump.
            dockerImage: "postgres:12-alpine"

            # Source of data.
            source:
              # Source type: local, remote, rds.
              type: local

              # Connection parameters of the database to be dumped.
              connection:
                dbname: postgres
                host: 127.0.0.1
                port: 5432
                username: postgres
                # Connection password. The environment variable PGPASSWORD can be used instead of this option.
                # The environment variable has a higher priority.
                password: postgres

              # Optional definition of RDS data source.
              rds:
                # RDS policy name.
                iamPolicyName: rds-dblab-retrieval

                # AWS Region.
                awsRegion: us-east-2

                # AWS username.
                username: db_user

                # RDS instance Identifier.
                dbInstanceIdentifier: database-1

                # Path to the SSL root certificate: https://s3.amazonaws.com/rds-downloads/rds-combined-ca-bundle.pem
                sslRootCert: "/tmp/rds-combined-ca-bundle.pem"

            # Options for a partial dump.
            partial:
              tables:
                - test

            # The number of parallel jobs to get a dump.
            # It's ignored if "restore" is present because "pg_dump | pg_restore" is always single-threaded.
            parallelJobs: 1

            # Options for direct restore to Database Lab Engine instance.
            restore:
              # Restore data even if the Postgres directory (`global.dataDir`) is not empty.
              # Note the existing data might be overwritten.
              forceInit: true

        # Restores PostgreSQL database from the provided dump.
        - name: logical-restore
          options:
            dbname: test
            # The location of the archive file (or directory, for a directory-format archive) to be restored.
            dumpLocation: /tmp/db.dump

            # Restore data even if the Postgres directory (`global.dataDir`) is not empty.
            # Note the existing data might be overwritten.
            forceInit: false

            # Options for a partial dump.
            partial:
              tables:
                - test

        # Restores database data from a physical backup.
        - name: physical-restore
          options:
            tool: walg
            dockerImage: "postgresai/sync-instance:12"
            envs:
              WALG_GS_PREFIX: "gs://{BUCKET}/{SCOPE}"
            walg:
              storage: gcs
              backupName: LATEST
              credentialsFile: /tmp/sa.json # optional

        - name: logical-snapshot
          options:
            # It is possible to define a pre-precessing script. For example, /tmp/scripts/custom.sh
            preprocessingScript: ""

            # Section for adding custom postgresql.conf parameters.
            configs:
              # In order to match production plans with Database Lab plans set parameters related to Query Planning as on production.

              # Be careful with memory consumption: besides shared_buffers (immediately allocated when the clone is started),
              # memory is consumed by Postgres backends directly (depends on work_mem and maintenance_work_mem values),
              # and by other applications.
              # If there is not enough memory to run more clones, then, depending on OS settings,
              # you may experience either OOM killer or swapping activity.
              # The "free" memory is automatically used for the file cache which is shared among clones.
              shared_buffers: 1GB
              # effective_cache_size: 64MB

        - name: physical-snapshot
          options:
            # Promote PGDATA after data fetching.
            promote: true

            # It is possible to define a pre-precessing script. For example, /tmp/scripts/custom.sh
            preprocessingScript: ""

            # Section for adding custom postgresql.conf parameters.
            configs:
              # In order to match production plans with Database Lab plans set parameters related to Query Planning as on production.
              shared_buffers: 1GB
              # effective_cache_size: 64MB

            # Scheduler contains tasks which run on a schedule.
            scheduler:
              # Snapshot scheduler creates a new snapshot on a schedule.
              snapshot:
                # Timetable defines in crontab format: https://en.wikipedia.org/wiki/Cron#Overview
                timetable: "0 */6 * * *"

              # Retention scheduler cleans up old snapshots on a schedule.
              retention:
                # Timetable defines in crontab format: https://en.wikipedia.org/wiki/Cron#Overview
                timetable: "0 * * * *"
                # Limit defines how many snapshots should be hold.
                limit: 4

debug: true
